1) What does DAG scheduler do? 
#  unresolved logical plan >  resolved logical plan (Catalog checks) >optimized plan(Catalyst optimizer) > physical model
***The above flow is done by Catalyst optimizer even before DAG scheduler enters***
>The DAG Scheduler takes the physical plan, breaks it into stages based on shuffle boundaries, and submits tasks to the Task Scheduler. It is part of driver program
>>Dataframe prespective
1) DataFrame / Dataset API (User Code)
   ↓
2) Logical Plan (Unresolved → Resolved)
   ↓
3) Catalyst Optimizer
   ↓
4) Physical Plan (SparkPlan)
   ↓
5) Tungsten Optimizations + WholeStageCodegen
   ↓
6) DAG Scheduler (creates stages at shuffle boundaries)
   ↓
7) Task Scheduler
   ↓
8) Executors run generated bytecode

>> RDD prespective

1) User RDD Code (Driver)
   ↓
2) RDD Lineage Graph (Logical DAG of RDDs)
   ↓
3) DAG Scheduler (creates stages at shuffle boundaries) (Driver)
   ↓
4) Task Scheduler (Driver)
   ↓
5) Executors
   ↓
6) Task Execution on Partitions

2) what is Logical Plan?
It is derived from :
> unresolved plan (Intial syntax tree of query/DF operations)
> analyzed plan(verifies query is valid against schema) 
> optimized plan(part of catalyst optimizer >>predicate push down > column pruning > join re-ordering > constant folding )

3) Explain Catalyst Optimizer components?
> Parser    : generates unresolved plan 
> Analyzer  : semantic checks , resolved lp
> Optimizer : optimized plan
> Planner   : creates cost based physical plan [Exchange, Hashaggregate, Sort]
> Code Generator : Generates java bytecode for query execution (wholestageQueryexecution) (via Tungsten) on top of physical plan

| Feature                 | DataFrame / SQL | RDD      |
| ----------------------- | --------------- | -----    |
| Catalyst optimizer      | ✅ Yes           | ❌ No  |
| Logical / physical plan | ✅ Yes           | ❌ No  |
| WholeStageCodegen       | ✅ Yes           | ❌ No  |
| Expression codegen      | ✅ Yes           | ❌ No  |
| User-defined lambdas    | ❌ No            | ✅ Yes |
| Black-box execution     | ❌ No            | ✅ Yes |


4) Stage components?
> Shuffle Map Stage
> Result Stage
> Stage retry logic >> done by DAG scheduler
> Stage dependencies

5) Task scheduler components?
> Handles locality [PROCESS_LOCAL, NODE_LOCAL, RACK_Local, ANY, NO_PREF]
> Retries failed tasks
> Task Lifecycle
> Task partition mapping
> Speculative Execution
> Task retry >> done by Task scheduler

6) Shuffle Mechanism:
>> Shuffle write
>During ShuffleMapStage map task process input partition 
>partitioning : based on shuffle key it determines which reducer partition each record belongs to
>spilling and bufferring : records are buffered in memory and spilled to disk if buffer fills up
>writing to executor disk the intermediate shuffle output, consists of data file and index file. 
>Data file contains serialized records grouped by partition destination and index contains data range  for each destination
>>Shuffle read 
>reduce tasks > fetch from multiple executors > merge > sort aggregate
>each reduce task is reponsible for processing  a specific partition of the shuffled data
>communicates with MapOutputTracker to get locations of these shuffle outputs.
>merging >> sorting >> aggregating

Sub topics:
1) Shuffle file format 2) MapOutputTracker 3) Push-based shuffle 4) Bypass merge sort 5) Memory vs Disk Spill 6) Skew handling
7) Salting 8) Broadcast Hash join 9) Adaptive Query Execution.

7) Spark UI components:
> Jobs Tab  : Number of stages per job / failed jobs 

> Stages Tab : shuffle read/ write, task duration , skew detection

> Tasks view : GC Time, serialization time, spill metrics

> Executors Tab : Memory usage, Disk Usage, Active Tasks






