>>> window_data = [
     ("Alice", "sales", "1000", "NY", "2023-01-01 10:00:00"),
     ("Bob", "HR", "1500", "NY", "2023-01-01 11:00:00"),
     ("Charlie", "IT", "2000", "NY", "2023-01 -01 12:00:00"),
     ("David", "sales", "2500", "NY", "2023-01-01 13:00:00"),
     ("Eve", "HR", "3000", "NY", "2023-01-01 14:00:00"),
     ("Frank", "IT", "3500", "NY", "2023-01-01 15:00:00"),
     ("Grace", "sales", "4000", "NY", "2023-01-01 16:00:00"),
     ("Heidi", "HR", "4500", "NY", "2023-01-01 17:00:00"),
     ("Ivan", "IT", "5000", "NY", "2023-01-01 18:00:00"),
     ("Judy", "sales", "5500", "NY", "2023-01-01 19:00:00"),
     ("Mallory", "HR", "6000", "NY", "2023-01-01 20:00:00"),
     ("Niaj", "IT", "6500", "NY", "2023-01-01 21:00:00"),
     ("Olivia", "HR", "7000", "NY", "2023-01-01 22:00:00"),
     ("Peggy", "sales", "7500", "NY", "2023-01-01 23:00:00"),
     ("Trent", "IT", "8000", "NY", "2023-01-02 00:00:00")]
>>> window_columns = ["name","dept","salary","city","timestamp"]
>>> window_df = spark.createDataFrame(window_data,window_columns)
>>> window_df.show()
+-------+-----+------+----+--------------------+
|   name| dept|salary|city|           timestamp|
+-------+-----+------+----+--------------------+
|  Alice|sales|  1000|  NY| 2023-01-01 10:00:00|
|    Bob|   HR|  1500|  NY| 2023-01-01 11:00:00|
|Charlie|   IT|  2000|  NY|2023-01 -01 12:00:00|
|  David|sales|  2500|  NY| 2023-01-01 13:00:00|
|    Eve|   HR|  3000|  NY| 2023-01-01 14:00:00|
|  Frank|   IT|  3500|  NY| 2023-01-01 15:00:00|
|  Grace|sales|  4000|  NY| 2023-01-01 16:00:00|
|  Heidi|   HR|  4500|  NY| 2023-01-01 17:00:00|
|   Ivan|   IT|  5000|  NY| 2023-01-01 18:00:00|
|   Judy|sales|  5500|  NY| 2023-01-01 19:00:00|
|Mallory|   HR|  6000|  NY| 2023-01-01 20:00:00|
|   Niaj|   IT|  6500|  NY| 2023-01-01 21:00:00|
| Olivia|   HR|  7000|  NY| 2023-01-01 22:00:00|
|  Peggy|sales|  7500|  NY| 2023-01-01 23:00:00|
|  Trent|   IT|  8000|  NY| 2023-01-02 00:00:00|
+-------+-----+------+----+--------------------+

>>> window_data = [
...      ("Alice", "sales", "1000", "NY", "2023-01-01 10:00:00"),
...      ("Bob", "HR", "1500", "CO", "2023-01-01 11:00:00"),
...      ("Charlie", "IT", "2000", "IN", "2023-01 -01 12:00:00"),
...      ("David", "sales", "2500", "NY", "2023-01-01 13:00:00"),
...      ("Eve", "HR", "3000", "NY", "2023-01-01 14:00:00"),
...      ("Frank", "IT", "3500", "IN", "2023-01-01 15:00:00"),
...      ("Grace", "sales", "4000", "SG", "2023-01-01 16:00:00"),
...      ("Heidi", "HR", "4500", "NY", "2023-01-01 17:00:00"),
...      ("Ivan", "IT", "5000", "NV", "2023-01-01 18:00:00"),
...      ("Bob", "sales", "1500", "SF", "2023-01-01 19:00:00"),
...      ("Mallory", "HR", "6000", "NY", "2023-01-01 20:00:00"),
...      ("Niaj", "IT", "6500", "NY", "2023-01-01 21:00:00"),
...      ("David", "HR", "2500", "CN", "2023-01-01 22:00:00"),
...      ("Alice", "sales", "1000", "SF", "2023-01-01 23:00:00"),
...      ("Trent", "IT", "8000", "NY", "2023-01-02 00:00:00")]
>>> window_columns = ["name","dept","salary","city","timestamp"]
>>> window_spec_name_time=Window.partitionBy("name").orderBy("timestamp")
>>> window_df = spark.createDataFrame(window_data,window_columns)
>>> window_df.withColumn("prev_salary",lag("salary",1,0).over(window_spec_name_time)).show()
+-------+-----+------+----+--------------------+-----------+
|   name| dept|salary|city|           timestamp|prev_salary|
+-------+-----+------+----+--------------------+-----------+
|  Alice|sales|  1000|  NY| 2023-01-01 10:00:00|          0|
|  Alice|sales|  1000|  SF| 2023-01-01 23:00:00|       1000|
|    Bob|   HR|  1500|  CO| 2023-01-01 11:00:00|          0|
|    Bob|sales|  1500|  SF| 2023-01-01 19:00:00|       1500|
|Charlie|   IT|  2000|  IN|2023-01 -01 12:00:00|          0|
|  David|sales|  2500|  NY| 2023-01-01 13:00:00|          0|
|  David|   HR|  2500|  CN| 2023-01-01 22:00:00|       2500|
|    Eve|   HR|  3000|  NY| 2023-01-01 14:00:00|          0|
|  Frank|   IT|  3500|  IN| 2023-01-01 15:00:00|          0|
|  Grace|sales|  4000|  SG| 2023-01-01 16:00:00|          0|
|  Heidi|   HR|  4500|  NY| 2023-01-01 17:00:00|          0|
|   Ivan|   IT|  5000|  NV| 2023-01-01 18:00:00|          0|
|Mallory|   HR|  6000|  NY| 2023-01-01 20:00:00|          0|
|   Niaj|   IT|  6500|  NY| 2023-01-01 21:00:00|          0|
|  Trent|   IT|  8000|  NY| 2023-01-02 00:00:00|          0|
+-------+-----+------+----+--------------------+-----------+

>>> window_df.withColumn("prev_salary",lead("salary",1,0).over(window_spec_name_time)).show()
+-------+-----+------+----+--------------------+-----------+
|   name| dept|salary|city|           timestamp|prev_salary|
+-------+-----+------+----+--------------------+-----------+
|  Alice|sales|  1000|  NY| 2023-01-01 10:00:00|       1000|
|  Alice|sales|  1000|  SF| 2023-01-01 23:00:00|          0|
|    Bob|   HR|  1500|  CO| 2023-01-01 11:00:00|       1500|
|    Bob|sales|  1500|  SF| 2023-01-01 19:00:00|          0|
|Charlie|   IT|  2000|  IN|2023-01 -01 12:00:00|          0|
|  David|sales|  2500|  NY| 2023-01-01 13:00:00|       2500|
|  David|   HR|  2500|  CN| 2023-01-01 22:00:00|          0|
|    Eve|   HR|  3000|  NY| 2023-01-01 14:00:00|          0|
|  Frank|   IT|  3500|  IN| 2023-01-01 15:00:00|          0|
|  Grace|sales|  4000|  SG| 2023-01-01 16:00:00|          0|
|  Heidi|   HR|  4500|  NY| 2023-01-01 17:00:00|          0|
|   Ivan|   IT|  5000|  NV| 2023-01-01 18:00:00|          0|
|Mallory|   HR|  6000|  NY| 2023-01-01 20:00:00|          0|
|   Niaj|   IT|  6500|  NY| 2023-01-01 21:00:00|          0|
|  Trent|   IT|  8000|  NY| 2023-01-02 00:00:00|          0|
+-------+-----+------+----+--------------------+-----------+

#solve problems on window functions like lag and lead and rank and dense rank